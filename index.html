<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sm4llVTONs: A Family of Specialized Virtual Try-On Models</title>
    <meta name="description" content="sm4llVTONs, a new family of highly efficient and specialized diffusion models for virtual try-on (VTON) applications, and more." />
    <meta name="keywords" content="sm4llVTONs, research papers, academic papers, AI research, hci, machine learning, published papers, scientific papers, academic journals, virtual try-on, digital fashion, VTON" />
    <link rel="icon" href="Assets/Favicon/icons8-research-color-16.png" sizes="16x16" type="image/png" />
    <link rel="icon" href="Assets/Favicon/icons8-research-color-32.png" sizes="32x32" type="image/png" />
    <link rel="icon" href="Assets/Favicon/icons8-research-color-96.png" sizes="96x96" type="image/png" />
    <link rel="stylesheet" type="text/css" href="style.css" />
  </head>
  <body>
    <nav class="nav">
      <a href="#abstract">Abstract</a>
      <a href="#introduction">Introduction</a>
      <a href="#methodology">Methodology</a>
      <a href="#results">Results</a>
      <a href="#outside-vton">Beyond VTON</a>
      <a href="#conclusion">Conclusion</a>
    </nav>

    <div class="header">
      <h1>sm4llVTONs: A Family of Specialized Virtual Try-On Models</h1>
      <div class="authors">
        <p>Andrea Baioni, Alex Puliatti</p>
        <p>
          <a href="mailto:andrea@andreabaioni.com">andrea@andreabaioni.com</a>,
          <a href="mailto:a@puliatti.com">a@puliatti.com</a>
        </p>
      </div>
    </div>

    <div class="buttons">
      <a href="#" class="disabled-button">Paper</a>
      <a href="#" target="_blank">Code</a>
      <a href="#" class="disabled-button">Dataset</a>
      <a href="#" class="disabled-button">Model</a>
      <a href="#bibtex">BibTex</a>
    </div>

    <div class="abstract" id="abstract">
      <h2>Abstract</h2>
      <p>sm4llVTONs (same methodology 4 all VTON) is a new family of highly efficient and specialized diffusion models for virtual try-on (VTON) applications, and more. This page provides an overview of our current models, methodology, and performance benchmarks. The full details of our training methodology and architecture will be published in our upcoming research paper. </p>
    </div>

    <div class="content-section" id="introduction">
      <h2>Introduction</h2>
      <h3>Our Current Models</h3>
      <p>The sm4llVTONs family consists of several lightweight models, each an expert in a specific VTON domain. This specialization allows them to achieve state-of-the-art results on relatively small, targeted datasets.</p>
      <div class="table-container">
        <table class="spreadsheet-table">
          <tr><th>Model Name</th><th>Task</th><th>Status</th></tr>
          <tr><td>sm4ll-eye</td><td>Sunglasses & Eyewear</td><td>Release</td></tr>
          <tr><td>sm4ll-shoes</td><td>Shoes & Footwear</td><td>Release</td></tr>
          <tr><td>sm4ll-face</td><td>Face Swapping</td><td>Beta</td></tr>
          <tr><td>sm4ll-top</td><td>Upper Body Garments</td><td>Alpha</td></tr>
          <tr><td>sm4ll-bottom</td><td>Lower Body Garments</td><td>Alpha</td></tr>
          <tr><td>sm4ll-dress</td><td>Dresses</td><td>Alpha</td></tr>
          <tr><td>sm4ll-bg</td><td>Background Replacement</td><td>Alpha</td></tr>
        </table>
      </div>
      <h3>Key Philosophy & Features</h3>
      <p>Our work is guided by a core philosophy that distinguishes it from general-purpose VTON and image editing models. Instead of a single, large model that handles many tasks, sm4llVTONs are experts fine-tuned for a single purpose. This results in higher fidelity, better detail preservation, and more intuitive control. Our methodology is built around a "train-like-you-infer" principle, ensuring that our models perform reliably on in-the-wild images, not just curated datasets.</p>
    </div>

    <div class="content-section" id="methodology">
      <h2>Methodology</h2>
      <p>While the complete methodology will be detailed in our paper, we can share a high-level overview of our three-stage process.</p>
      <h3>Data Curation & Preparation</h3>
      <p>The model is trained on a dataset constructed from paired image sources and various augmentation techniques, such as cropping to mask size, automasking, etc. To improve robustness to real-world use cases, we employ a inference-aware masking strategy during the training phase, i.e.: instead of applying the best possible masking pipeline specific to the dataset, we employ the best possible masking pipeline that would be applied during inference instead during training as well. This way, we are able to generate masks that simulate user behaviour in production pipelines, instead of relying on best case scenarios that try to replicate training conditions during inference. This enhances the model's robustness against both variance and artifacts common in user-uploaded content.</p>
      <h3>The Training Process</h3>
      <p>All models in the sm4llVTONs family are trained using a unified methodology derived from foundational instruction-based models. We have introduced significant modifications to the training loop, including an optimized loss calculation. This technique focuses the model’s learning exclusively on the relevant image regions, which dramatically improves sample efficiency and the quality of the final result.</p>
      <h3>Inference & Validation</h3>
      <p>Inference is handled via ComfyUI, in an environment that mirrors the training conditions. Our most critical component is a custom, and tailored to the model’s objective, automasking process that is consistent between training and real-world use. This end-to-end consistency is fundamental to our models' success and addresses a common pitfall in VTON systems, where models fail to generalize because their training data does not reflect real-world input.</p>
    </div>

    <div class="content-section" id="results">
      <h2>Results</h2>

      <div class="category-section reveal-on-scroll">
        <h3>Glasses</h3>
        <div class="image-gallery-grid">
          <img src="Assets/glass_1.jpg" alt="Glasses Example 1">
          <img src="Assets/glass_2.jpg" alt="Glasses Example 2">
          <img src="Assets/glass_3.jpg" alt="Glasses Example 3">
          <img src="Assets/glass_4.jpg" alt="Glasses Example 4">
        </div>
      </div>

      <div class="category-section reveal-on-scroll">
        <h3>Top</h3>
        <div class="image-gallery-grid">
          <img src="Assets/top_1.jpg" alt="Top Garment Example 1">
          <img src="Assets/top_2.jpg" alt="Top Garment Example 2">
          <img src="Assets/top_3.jpg" alt="Top Garment Example 3">
          <img src="Assets/top_4.jpg" alt="Top Garment Example 4">
        </div>
      </div>

      <div class="category-section reveal-on-scroll">
        <h3>Bottom</h3>
        <div class="image-gallery-grid">
          <img src="Assets/bottom_1.jpg" alt="Bottom Garment Example 1">
          <img src="Assets/bottom_2.jpg" alt="Bottom Garment Example 2">
          <img src="Assets/bottom_3.jpg" alt="Bottom Garment Example 3">
          <img src="Assets/bottom_4.jpg" alt="Bottom Garment Example 4">
        </div>
      </div>

      <div class="category-section reveal-on-scroll">
        <h3>Dresses</h3>
        <div class="image-gallery-grid">
          <img src="Assets/dress_1.jpg" alt="Dress Example 1">
          <img src="Assets/dress_2.jpg" alt="Dress Example 2">
          <img src="Assets/dress_3.jpg" alt="Dress Example 3">
          <img src="Assets/dress_4.jpg" alt="Dress Example 4">
        </div>
      </div>

      <div class="category-section reveal-on-scroll">
        <h3>Shoes</h3>
        <div class="image-gallery-grid">
          <img src="Assets/shoes_1.jpg" alt="Shoes Example 1">
          <img src="Assets/shoes_2.jpg" alt="Shoes Example 2">
          <img src="Assets/shoes_3.jpg" alt="Shoes Example 3">
          <img src="Assets/shoes_4.jpg" alt="Shoes Example 4">
        </div>
      </div>

      <div class="category-section reveal-on-scroll">
        <h3>Qualitative Evaluations</h3>
        <div class="qualitative-text">
            <p>
              While we tried our best in getting <strong>Flux Kontext</strong> to deliver acceptable results, we didn't find a consistent prompt structure that could output the desired type of image modifications consistently. As such, these results are the best we could generate, but we don't necessarily think they are indicative of what the model could be capable of in the hands of Black Forest Labs. It is, anyway, in the very nature of instruct-based models to be somewhat limited in their ability to properly execute on prompts written by users who either don't have access to the training dataset, or have limited access to proper documentation.
            </p>
            <p>
              On another note, OpenAI's <strong>GPT-4o</strong> is probably the best model in terms of aesthetic scoring, but, as all current autoregressive models do, "cheats" by delivering an image that is not a direct modification of neither of the two input images, but rather a third, similar image, that cannot be used in a production environment, as even small changes in product and / or underlying approved assets is usually not acceptable in a creative production pipeline.
            </p>
            <p>
              In terms of autosegmentation, <strong>FASHN.ai</strong> is the only one amongst the benchmarked models that tries to have a predictive approach to the segmentation problem. As such, some of the worst generations from FASHN are not to be taken as a limitation of the underlying model, but most likely than not, a limitation of the autosegmentation system employed itself.
            </p>
            <p>
              <strong>CatVTON Flux</strong> still performs rather well in terms of generalization, even when considering that the dataset it's based off is DressCode, which is optimized for Top, Bottom, and Full Body garments only. Regardless of this, it still can perform decently when tasked with out of scope items, such as eyewear and shoes.
            </p>
            <p>
              Although these benchmarks are from a preliminary testing phase and a more in-depth evaluation is ongoing, the sm4ll model family has demonstrated consistently stronger performance compared to the other models in the following areas:
            </p>
            <ul>
              <li>
                delivering accurate results through models that are expert in specific product categories, which is both an (unfair) advantage deriving from the methodology itself, and a desirable outcome in terms of real-world creative production pipelines
              </li>
              <li>
                being able to minimally affect the underlying input image(s) while retaining product precision, which is, in our experience, a relevant concern in real-world production pipelines.
              </li>
            </ul>
        </div>
      </div>
    </div>
    
    <div class="content-section reveal-on-scroll" id="outside-vton">
      <h2>Outside of traditional VTON use-cases</h2>
      <h3>Face-swap</h3>
      <div class="qualitative-text">
          <p>
            Currently in its initial stages, <strong>sm4ll-face</strong> has undergone preliminary testing against best-in-class, publicly available face swapping models: <strong>ACE++</strong> and <strong>ReActor</strong>.
          </p>
          <p>
            From the early tests we have conducted so far, while <strong>ACE++</strong> came close to our results in terms of scoring and qualitative similarity, it struggled with adapting the subject's light based on the context around it (see example 1 and example 2, a cold and blue-ish light coming from our subject's picture). Instead, <strong>sm4ll-face</strong> reaches a slightly higher FID score in around 60% of the benchmark generations while achieving consistently a good relighting of the subject's face based on the surrounding context.
          </p>
          <p>
            While preliminary tests performed with the Beta version of <strong>sm4ll-face</strong> model haven't been consistently surpassing <strong>ACE++</strong> and <strong>ReActor</strong> in <em>FID</em> and <em>CLIP</em> scores 100% of the times, we will run more comprehensive quantitative tests once the model is ready for release.
          </p>
      </div>
      <div class="image-gallery-grid">
        <img src="Assets/face_1.jpg" alt="Face-swap Example 1">
        <img src="Assets/face_2.jpg" alt="Face-swap Example 2">
        <img src="Assets/face_3.jpg" alt="Face-swap Example 3">
        <img src="Assets/face_4.jpg" alt="Face-swap Example 4">
      </div>
    </div>

    <div class="content-section" id="conclusion">
      <h2>Conclusion</h2>
      <p>The sm4llVTONs family of models represents a significant step forward in the field of specialized virtual try-on applications. By focusing on lightweight, expert models and a "train-as-you-infer" methodology, we achieve high-fidelity results that are robust to real-world conditions. Future work will involve the release of the full research paper with detailed benchmarks and continued development of the alpha and beta models.</p>
    </div>

    <div class="bibtex-section" id="bibtex">
      <h2>BibTeX</h2>
      <button class="bibtex-copy-button" onclick="copyBibTeX()">Copy to Clipboard</button>
      <pre><code class="language-bibtex">@inproceedings{sm4llVTONs2025,
  title={sm4llVTONs: A Family of Specialized Virtual Try-On Models},
  author={Andrea Baioni and Alex Puliatti},
  year={2025}
}</code></pre>
    </div>

    <div class="content-section" id="acknowledgement">
      <h2>Acknowledgement</h2>
      <div class="references">
        <p>This work builds upon the foundational research of many, including:</p>
        <p>Brooks, T., Holynski, A. and Efros, A.A. (2023). <a href="https://arxiv.org/abs/2211.09800" target="_blank"><em>InstructPix2Pix: Learning to Follow Image Editing Instructions</em></a>. arXiv preprint arXiv:2211.09800.</p>
        <p>Chong, Z., et al. (2025). <a href="https://arxiv.org/abs/2407.15886" target="_blank"><em>CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models</em></a>. arXiv preprint arXiv:2407.15886.</p>
        <p>Huang, L., et al. (2024). <a href="https://arxiv.org/abs/2410.23775" target="_blank"><em>In-Context LoRA for Diffusion Transformers</em></a>. arXiv preprint arXiv:2410.23775.</p>
      </div>
    </div>

    <div class="footer">
      <p>© 2025 [sm4llVTONs Team]. All rights reserved.</p>
      <p>
        Website template free to borrow from
        <a href="https://github.com/indramal/iNdra-GitHub-Page-Template-For-Resarch">here</a>.
      </p>
    </div>

    <button class="scrollUpBtn" id="scrollUpBtn" onclick="scrollToTop()">⬆</button>
    
    <div id="imageModal" class="modal">
      <span class="close-modal" id="closeModalSpan">&times;</span>
      <img class="modal-content" id="modalImage">
    </div>

    <script src="script.js"></script>
  </body>
</html>
